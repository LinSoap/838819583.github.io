{"meta":{"title":"LinSoap","subtitle":"林肥皂的博客","description":"","author":"林肥皂","url":"https://838819583.github.io","root":"/"},"pages":[{"title":"文章归档","date":"2022-10-25T03:45:53.928Z","updated":"2022-10-25T03:45:53.928Z","comments":true,"path":"archives.html","permalink":"https://838819583.github.io/archives.html","excerpt":"","text":""}],"posts":[{"title":"整理一下正在使用的好用开源工具","slug":"整理一下正在使用的好用开源软件","date":"2023-09-23T13:54:31.000Z","updated":"2023-09-23T15:57:51.517Z","comments":true,"path":"2023/09/23/整理一下正在使用的好用开源软件/","link":"","permalink":"https://838819583.github.io/2023/09/23/%E6%95%B4%E7%90%86%E4%B8%80%E4%B8%8B%E6%AD%A3%E5%9C%A8%E4%BD%BF%E7%94%A8%E7%9A%84%E5%A5%BD%E7%94%A8%E5%BC%80%E6%BA%90%E8%BD%AF%E4%BB%B6/","excerpt":"最近发现了很多开源工具用起来很顺手，也学生优惠白嫖了一个华为云，在上面跑了一些服务，在此总结一下","text":"最近发现了很多开源工具用起来很顺手，也学生优惠白嫖了一个华为云，在上面跑了一些服务，在此总结一下 Linux1.Alist 网盘整合Web一个可以整合多个资源的Web工具，包含阿里云、本地存储、OneDrive等等，支持在线查看PDF，视频等格式，还支持挂载，总之非常强大，本人用学校的教育邮箱白嫖了1T的OneDrive网盘，平时把代码备份到OneDrive，在机房上课偷偷看PDF还是很香的 2.Chevereto 图床Web发现这个项目后，才发现之前一直用的路过图床其实就是魔改的这个项目，果断部署一个，不过传输速度受限于华为云的水管带宽，不过用于插入Markdown图片还是够用的。 3.JupyterNoteBook 在线Python编程Web平时写一些数据分析或者小项目的时候经常会用JupyterNoteBook，最近突然发现可以以服务的形式部署到服务器上，只需要用浏览器访问就能直接开始用，而且支持在代码中添加Markdown，读写很友好，最近一直用这个上课无聊时刷刷LeetCode，写写经验非常好用。 WindowsWindows作为主力环境，在各种工具加持下用的特别舒心 1.Snipaste 截图贴图工具用了很久的一个工具了，可以直接截图并贴图在屏幕上，适合用于抄代码或者看文档，而且设置无边框贴图后，可以直接用贴图工具进行拼接创造 2.Everything 高效文件查找工具如果你用过windows自带的文件查找功能，那你肯定会觉得微软那帮人是不是在偷懒，查找一个文件夹中的内容都慢的要死，相比之下，Everything可以快速寻找整个磁盘中的内容 3.Wox 快捷启动和文件查找小组件Wox其实依赖于Everything，在原先的基础上添加了快捷键启动的方式，得益于Everything的效率，比在桌面一大堆文件夹中寻找某个图标双击的效率还高，Alt+Space，敲入’明日方舟‘，回车，启动！ 4.MarkText 优雅的Markdown编写工具之前用过VsCode编写Markdown，但除了Markdown语法，还要学一些VsCode的配置和使用方式，每次学会了，许久不写之后又忘了非常的烦，MarkText直接启动就能编写，并且能够实时渲染，非常友好 5.TrafficMonitor 网速、CPU、内存监视小组件可以直接固定在任务栏上的一个硬件状态监视工具，可以看看当前网络的上传和下载情况，以及内存和CPU负载情况，在于，但是这个组件的CPU负载情况显示其实是有问题的（至少对于我4800H是这样的，常年显示低于10%） 6.PyBingWallpaper 自动更换Bing壁纸我还是很喜欢windows每日会推送开机界面的行为，有种每日开盲盒的感觉，虽然之前一直用wallpaper动态壁纸，壁纸质量很高，但是总会腻，换新壁纸又很懒，这个服务可以每天定时更换壁纸，由于壁纸源自Bing，质量也是非常高的。 7.TranslucentTB 任务栏透明工具既然每日换上了好看的壁纸，但是任务栏颜色不会跟着改变，通过这个工具可以使得任务栏透明，但是任务栏上的图标还是在的，所以不影响任务栏的便携性，还能够根据不同的使用场景设置不同的透明度，利用Wox清空桌面后，再加上透明的任务栏，切换到桌面欣赏欣赏每日壁纸非常的享受~~ 8.TTime 一键翻译工具对于每日需要翻英文文档的程序员，有时候使用google自带的翻译会导致浏览器渲染错误，TTime支持划词翻译，截图ORC翻译，想翻译为英文的时候，也可以ALT+E一键唤出翻译。 9.listen1 音乐集成播放器之前一直用网易云音乐听歌，但是网易的日推也太土了，天天推抖音神曲，资源也不算太多，想直接听专辑入口也很难找。这个播放器集成了网易云，qq音乐，和虾米音乐登录，搜索集成的平台更多，界面也很符合我的审美。 10.SpaceSniffer 磁盘分析工具这个软件虽然不是开源的，但是免费使用，也比较强大，能够可视化的分析一个磁盘使用的情况，非常直观的看到各个文件的占用情况","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://838819583.github.io/tags/linux/"},{"name":"开源工具","slug":"开源工具","permalink":"https://838819583.github.io/tags/%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7/"}],"author":"林肥皂"},{"title":"grub.cfg配置错误开机进入dracut修复","slug":"grub-cfg配置错误开机进入dracut修复","date":"2022-10-24T03:39:55.000Z","updated":"2022-10-24T04:54:55.602Z","comments":true,"path":"2022/10/24/grub-cfg配置错误开机进入dracut修复/","link":"","permalink":"https://838819583.github.io/2022/10/24/grub-cfg%E9%85%8D%E7%BD%AE%E9%94%99%E8%AF%AF%E5%BC%80%E6%9C%BA%E8%BF%9B%E5%85%A5dracut%E4%BF%AE%E5%A4%8D/","excerpt":"记录一次作死后更改grub.cfg抢救经历","text":"记录一次作死后更改grub.cfg抢救经历 前几天打算使用学校服务器虚拟化一台windows玩玩，但是发现kvm虚拟出的windows显卡性能很低并且无法安装驱动，所以尝试使用显卡直通，网络上搜索了教程，其中有一步是修改&#x2F;etc&#x2F;default&#x2F;grub，我没有注意误将rd.lvm.lv配置修改并执行了 grub2-mkconfig -o &#x2F;boot&#x2F;efi&#x2F;EFI&#x2F;centos&#x2F;grub.cfg 重启后直接进入dracut模式并提示系统提示&#x2F;dev&#x2F;mapper&#x2F;centos00-root dose not exist，其中&#x2F;dev&#x2F;mapper路径下放置了VG(volume group)和PV(physcial volume)的映射关系，由于我修改了grub.cfg导致开机时映射关系丢失。 这是我原磁盘分区方式其中我做了双系统，主要是系统为centos00，为lvm分区格式，在&#x2F;dev目录下只有centos&#x2F;root，并没有centos00&#x2F;root，原因是未激活该逻辑卷。所以要先激活该逻辑卷。 1234567891011121314151617181920212223#由于dracut并没有需要使用到的vgsan和vgchange指令，所以先挂载centos mkdir /tmp1mount /dev/centos/root /tmp1# vgscan 扫描逻辑盘vgscan#Reading all physical volumes. This may take a while...#Found volume group &quot;centos00&quot; using metadata type lvm2#Found volume group &quot;centos&quot; using metadata type lvm2#激活centos00逻辑卷vgchange -a y centos00#挂载centos00mkdir /mntmount /dev/centos00/root /mnt#挂载/bootmount /dev/sdb2 /mnt/boot#挂载/boot/efimount /dev/sdb1 /mnt/boot/efifor i in /dev /proc /sys;do mount -B $i /mnt$i;done 至此已经可以访问到原系统的内容了，只需要进入系统修改&#x2F;etc&#x2F;default&#x2F;grub重新执行grub2-mkconfig即可 123456#使用chroot改变系统路径进入chroot /mnt#修改grub文件vim /etc/default/grub#重新执行grub2-mkconfiggrub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg 重启检验是否能正常开机 容易出现的错误：一. basename: command not found原因:未使用chroot进入&#x2F;mnt，使得grub2-mkconfig脚本无法正确获取到需要使用的脚本。解决：只需chroot &#x2F;mnt再执行命令即可 二. failed to get canomical path of ‘rootfs’原因:未挂载&#x2F;sys&#x2F;fs，导致脚本无法获取到根系统。解决：将所有脚本可能使用的目录挂载上 1for i in /dev /proc /sys;do mount -B $i /mnt$i;done","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://838819583.github.io/tags/linux/"}],"author":"林肥皂"},{"title":"Pyecharts各图输入数据格式","slug":"Pyechars各图输入数据格式","date":"2022-05-07T02:31:35.000Z","updated":"2022-10-20T09:10:18.626Z","comments":true,"path":"2022/05/07/Pyechars各图输入数据格式/","link":"","permalink":"https://838819583.github.io/2022/05/07/Pyechars%E5%90%84%E5%9B%BE%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/","excerpt":"整理了一下Pyechats常见的图标输入数据格式","text":"整理了一下Pyechats常见的图标输入数据格式 最近在学习使用pyecharts做一些数据可视化，pyechars在制作各种图标的时候流程几乎一样，但是对于输入数据的格式要求不同，官方文档描述的也不清楚，所有在此重新记录和梳理一下各图表的输入数据格式。 直角坐标系类图形直角坐标系类图表包括柱状图\\条形图(Bar)、箱型图(Boxplot)、涟漪特效散点图(EffectScatter),热力图(HeatMap)，折线图\\面积图(Line),象形柱状图(PictorialBar),散点图(Scatter)，以及k线图(Kline)。直角坐标系最重要的就是x、y轴的设置了，在这类图表中，都有add_xaxis和add_yaxis方法，所有图表add_xaxis的输入都相同，都是包含x轴坐标标签的列表，但是对于不同图表，add_yaxis的方法会有所不同。 柱状\\条形图、涟漪特效散点图、折线\\面积图、象形柱状图、散点图观察函数签名BarEffectScatterLine PictorialBarScatter 这些图表对于add_yaxis的要求都是一个包含y数据的列表或者包含该图表item的列表，其中y值只有一个。输入类型一 1ydata = [143, 147, 112, 105, 87, 43, 127] 输入类型二(以Bar为例) 123data = [opts.BarItem(name=&quot;商家A&quot;,value=143), opts.BarItem(name=&quot;商家A&quot;,value=147), opts.BarItem(name=&quot;商家A&quot;,value=112)] 箱型图先观察函数签名，发现输入要求貌似与上面那些图表相同再观察opts.BoxplotItem函数前面,就会发现item的value要求输入一个Sqequence，所以箱型图的输入数据还是与以上图表不同。输入类型一 1234data = [ [850, 740, 900, 1070, 930, 850, 950, 980,980,880, 1000, 980], [960, 940, 960, 940, 880, 800, 850, 880, 900, 840, 830, 790],] 输入类型二 12data = [opts.BoxplotItem(&quot;商家A&quot;,value=[850, 740, 900, 1070, 930, 850, 950, 980,980, 880, 1000, 980]), opts.BoxplotItem(&quot;商家A&quot;,value=[960, 940, 960, 940, 880, 800, 850, 880, 900, 840, 830, 790])] 热力图热力图的add_yaxis方法就不支持item的形式添加数据，并且还多了一个yaxis_data参数，该参数表示y坐标的标签，而value值是一个包含三个数据的列表，前两个数据表示该值的xy坐标，第三个数据才是y值。 输入类型 12345678910111213yaxis_data = [&#x27;春&#x27;, &#x27;夏&#x27;, &#x27;秋&#x27;,&#x27;冬&#x27;]value = [[0, 0, 28], [0, 1, 25], [0, 2, 25], [0, 3, 38], [1, 0, 3], [1, 1, 6], [1, 2, 21], [1, 3, 27], [2, 0, 48], [2, 1, 20], [2, 2, 30], [2, 3, 15]] 键值对类图形饼图输入类型一 1234567data = [[&#x27;可乐&#x27;, 39], [&#x27;雪碧&#x27;, 78], [&#x27;橙汁&#x27;, 53], [&#x27;绿茶&#x27;, 59], [&#x27;奶茶&#x27;, 102], [&#x27;百威&#x27;, 126], [&#x27;青岛&#x27;, 31]] 输入类型二(该输入会导致Legend不显示) 1234567data = [opts.PieItem(&quot;可乐&quot;,value=39), opts.PieItem(&quot;雪碧&quot;,value=78), opts.PieItem(&quot;橙汁&quot;,value=53), opts.PieItem(&quot;绿茶&quot;,value=59), opts.PieItem(&quot;奶茶&quot;,value=102), opts.PieItem(&quot;百威&quot;,value=126), opts.PieItem(&quot;青岛&quot;,value=31)] 漏斗图 1234567data = [[&#x27;草莓&#x27;, 85], [&#x27;芒果&#x27;, 52], [&#x27;葡萄&#x27;, 56], [&#x27;雪梨&#x27;, 92], [&#x27;西瓜&#x27;, 72], [&#x27;柠檬&#x27;, 110], [&#x27;车厘子&#x27;, 23]] 仪表盘 1data = [[&quot;完成率&quot;, 55.5]] 词云 123456789data = [(&quot;生活资源&quot;, &quot;999&quot;), (&quot;供热管理&quot;, &quot;888&quot;), (&quot;供气质量&quot;, &quot;777&quot;), (&quot;生活用水管理&quot;, &quot;688&quot;), (&quot;一次供水问题&quot;, &quot;588&quot;), (&quot;交通运输&quot;, &quot;516&quot;), (&quot;城市交通&quot;, &quot;515&quot;), (&quot;环境保护&quot;, &quot;483&quot;), (&quot;房地产管理&quot;, &quot;462&quot;)] 日历图 123456789101112131415import datetimebegin = datetime.date(2017, 1, 1)end = datetime.date(2017, 12, 31)data = [ [str(begin + datetime.timedelta(days=i)), random.randint(1000, 25000)] for i in range((end - begin).days + 1)]/**输出结果 *[[&#x27;2017-01-01&#x27;, 11163], *[&#x27;2017-01-02&#x27;, 1527], *[&#x27;2017-01-03&#x27;, 20727], *[&#x27;2017-01-04&#x27;, 20768], *[&#x27;2017-01-05&#x27;, 4260], *[&#x27;2017-01-06&#x27;, 1655]] */ 树型结构类型图树图输入类型一 12345678910111213141516171819data = [ &#123; &quot;children&quot;: [ &#123;&quot;name&quot;: &quot;B&quot;&#125;, &#123; &quot;children&quot;: [&#123;&quot;children&quot;: [&#123;&quot;name&quot;: &quot;I&quot;&#125;], &quot;name&quot;: &quot;E&quot;&#125;, &#123;&quot;name&quot;: &quot;F&quot;&#125;], &quot;name&quot;: &quot;C&quot;, &#125;, &#123; &quot;children&quot;: [ &#123;&quot;children&quot;: [&#123;&quot;name&quot;: &quot;J&quot;&#125;, &#123;&quot;name&quot;: &quot;K&quot;&#125;], &quot;name&quot;: &quot;G&quot;&#125;, &#123;&quot;name&quot;: &quot;H&quot;&#125;, ], &quot;name&quot;: &quot;D&quot;, &#125;, ], &quot;name&quot;: &quot;A&quot;, &#125;] 输入类型二 1234567891011data = [TreeItem(name=&quot;A&quot;, children=[TreeItem(name=&quot;B&quot;), TreeItem(name=&quot;C&quot;), TreeItem(name=&quot;D&quot;, children=[TreeItem(name=&quot;E&quot;), TreeItem(name=&quot;F&quot;) ] ) ] ) ] 桑基图 123456789101112131415nodes = [ &#123;&quot;name&quot;: &quot;category1&quot;&#125;, &#123;&quot;name&quot;: &quot;category2&quot;&#125;, &#123;&quot;name&quot;: &quot;category3&quot;&#125;, &#123;&quot;name&quot;: &quot;category4&quot;&#125;, &#123;&quot;name&quot;: &quot;category5&quot;&#125;, &#123;&quot;name&quot;: &quot;category6&quot;&#125;,]links = [ &#123;&quot;source&quot;: &quot;category1&quot;, &quot;target&quot;: &quot;category2&quot;, &quot;value&quot;: 10&#125;, &#123;&quot;source&quot;: &quot;category2&quot;, &quot;target&quot;: &quot;category3&quot;, &quot;value&quot;: 15&#125;, &#123;&quot;source&quot;: &quot;category3&quot;, &quot;target&quot;: &quot;category4&quot;, &quot;value&quot;: 20&#125;, &#123;&quot;source&quot;: &quot;category5&quot;, &quot;target&quot;: &quot;category6&quot;, &quot;value&quot;: 25&#125;,] 关系图输入方式一 12345678910111213141516171819202122nodes = [ &#123;&quot;name&quot;: &quot;结点1&quot;, &quot;symbolSize&quot;: 10&#125;, &#123;&quot;name&quot;: &quot;结点2&quot;, &quot;symbolSize&quot;: 20&#125;, &#123;&quot;name&quot;: &quot;结点3&quot;, &quot;symbolSize&quot;: 30&#125;, &#123;&quot;name&quot;: &quot;结点4&quot;, &quot;symbolSize&quot;: 40&#125;, &#123;&quot;name&quot;: &quot;结点5&quot;, &quot;symbolSize&quot;: 50&#125;, &#123;&quot;name&quot;: &quot;结点6&quot;, &quot;symbolSize&quot;: 40&#125;, &#123;&quot;name&quot;: &quot;结点7&quot;, &quot;symbolSize&quot;: 30&#125;, &#123;&quot;name&quot;: &quot;结点8&quot;, &quot;symbolSize&quot;: 20&#125;,]links = [&#123;&#x27;source&#x27;: &#x27;结点1&#x27;, &#x27;target&#x27;: &#x27;结点1&#x27;&#125;, &#123;&#x27;source&#x27;: &#x27;结点1&#x27;, &#x27;target&#x27;: &#x27;结点2&#x27;&#125;, &#123;&#x27;source&#x27;: &#x27;结点1&#x27;, &#x27;target&#x27;: &#x27;结点3&#x27;&#125;, &#123;&#x27;source&#x27;: &#x27;结点1&#x27;, &#x27;target&#x27;: &#x27;结点4&#x27;&#125;, &#123;&#x27;source&#x27;: &#x27;结点1&#x27;, &#x27;target&#x27;: &#x27;结点5&#x27;&#125;, &#123;&#x27;source&#x27;: &#x27;结点1&#x27;, &#x27;target&#x27;: &#x27;结点6&#x27;&#125;, &#123;&#x27;source&#x27;: &#x27;结点1&#x27;, &#x27;target&#x27;: &#x27;结点7&#x27;&#125;, &#123;&#x27;source&#x27;: &#x27;结点1&#x27;, &#x27;target&#x27;: &#x27;结点8&#x27;&#125;, &#123;&#x27;source&#x27;: &#x27;结点2&#x27;, &#x27;target&#x27;: &#x27;结点1&#x27;&#125;, &#123;&#x27;source&#x27;: &#x27;结点2&#x27;, &#x27;target&#x27;: &#x27;结点2&#x27;&#125;, &#123;&#x27;source&#x27;: &#x27;结点2&#x27;, &#x27;target&#x27;: &#x27;结点3&#x27;&#125; ] 输入方式二 12345678910nodes = [GraphNode(name=&quot;结点1&quot;), GraphNode(name=&quot;结点2&quot;), GraphNode(name=&quot;结点3&quot;), GraphNode(name=&quot;结点4&quot;) ]links = [GraphLink(source=&quot;结点1&quot;,target=&quot;结点2&quot;), GraphLink(source=&quot;结点1&quot;,target=&quot;结点3&quot;), GraphLink(source=&quot;结点1&quot;,target=&quot;结点4&quot;), GraphLink(source=&quot;结点2&quot;,target=&quot;结点3&quot;), ] 地图地图输入方式一 1234567data = [[&#x27;广东&#x27;, 27], [&#x27;北京&#x27;, 89], [&#x27;上海&#x27;, 150], [&#x27;江西&#x27;, 146], [&#x27;湖南&#x27;, 60], [&#x27;浙江&#x27;, 61], [&#x27;江苏&#x27;, 132]] 输入方式二 1234567data = [MapItem(name=&quot;广东&quot;,value=27), MapItem(name=&quot;北京&quot;,value=89), MapItem(name=&quot;上海&quot;,value=150), MapItem(name=&quot;江西&quot;,value=146), MapItem(name=&quot;湖南&quot;,value=60), MapItem(name=&quot;浙江&quot;,value=61) ]","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://838819583.github.io/tags/python/"},{"name":"pyecharts","slug":"pyecharts","permalink":"https://838819583.github.io/tags/pyecharts/"},{"name":"可视化","slug":"可视化","permalink":"https://838819583.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"}],"author":"林肥皂"},{"title":"Flink HA配置","slug":"Flink-HA配置","date":"2022-04-14T06:19:42.000Z","updated":"2022-10-20T03:36:11.829Z","comments":true,"path":"2022/04/14/Flink-HA配置/","link":"","permalink":"https://838819583.github.io/2022/04/14/Flink-HA%E9%85%8D%E7%BD%AE/","excerpt":"Flink HA安装配置记录","text":"Flink HA安装配置记录 Flink HA时，首先确保你的Zookeeper和Hadoop已经安装完成，安装过程查看博客Hadoop集群安装与配置Zookeeper安装配置 Flink HA的安装配置主要分为两步1、解压flink.tar.gz，完成环境配置2、修改masters、slaves、flink-conf.yaml 1、解压flink.tar.gz，完成环境配置123456tar -zxf flink.tar.gz -C /opt/mv /opt/flink-1.10.2 /opt/flinkvim /etc/profile/export FLINK_HOME=/opt/flinkexport PATH=$PATH:$FLINK_HOME/binsource /etc/profile 2、修改masters、slaves、flink-conf.yaml123456789101112131415vim /opt/flink/conf/mastersmastersslave1#修改slavesvim /opt/flink/conf/slavesslave1slave2#修改flink-conf.yamlvim /opt/flink/conf/flink-conf.yaml#注释掉 jobmanager.rpc.address#取消注释high-availability,high-availability.storageDir,high-availability.zookeeper.quorum#修改HA相关配置high-availability: zookeeperhigh-availability.storageDir: hdfs:///flinkha/high-availability.zookeeper.quorum: master:2181,slave1:2181,slave2:2181 配置完成后在master节点上执行start-cluster.sh master slave1 slave2 StandaloneSessionClusterEntrypoint √ √ TaskManagerRunner √ √ √","categories":[],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://838819583.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"flink","slug":"flink","permalink":"https://838819583.github.io/tags/flink/"}],"author":"林肥皂"},{"title":"Spark HA安装配置","slug":"Spark-HA安装配置","date":"2022-04-14T05:27:39.000Z","updated":"2022-04-19T08:27:16.764Z","comments":true,"path":"2022/04/14/Spark-HA安装配置/","link":"","permalink":"https://838819583.github.io/2022/04/14/Spark-HA%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","excerpt":"Spark HA安装配置记录","text":"Spark HA安装配置记录 spark HA时，首先确保你的Zookeeper和Hadoop已经安装完成，安装过程查看博客Hadoop集群安装与配置Zookeeper安装配置 Spark HA的安装配置主要分为三步1、解压tar包，配置环境变量2、配置JAVA_HOME,HADOOP_HOME,HADOOP_CONF_DIR,HADOOP_CLASSPATH3、修改slaves文件,spark.env.sh 1、解压tar包，配置环境变量123456tar -zxf spark.tar.gz -C /optmv /opt/spark-2.1.2 /opt/sparkvim /etc/profileexport SPARK_HOME=/opt/sparkexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbinsource /etc/profile 2、配置JAVA_HOME,HADOOP_HOME,HADOOP_CONF_DIR,HADOOP_CLASSPATH12345678vim /etc/profileexport JAVA_HOME=/usr/local/src/jdk1.8.0_291/export PATH=$PATH:$JAVA_HOME/binexport HADOOP_HOME=/opt/hadoop/export HADOOP_CLASSPATH=`/opt/hadoop/bin/hadoop classpath`export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinsource /etc/profile 修改slaves文件,spark.env.sh1234567891011cp /opt/spark/conf/slaves.template /opt/spark/conf/slavesvim /opt/spark/conf/slaves#添加Worker节点masterslave1slave2cp /opt/spark/conf/spark-env.sh.template /opt/spark/conf/spark-env.shvim /opt/spark/conf/spark-env.sh#修改SPARK_DAEMON_JAVA_OPTSexport SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.depoly.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper. url=master1:2181,slave1:2181,slave2:2181 -Dspark.deploy.zookeeper.dir=/spark&quot; 完成配置后，进入sbin目录下，在master执行.&#x2F;spark-all.sh，此时会在master上启动Master节点，再在Slave1上执行start-master.sh master slave1 slave2 Master √ √ Worker √ √ √","categories":[],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://838819583.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"spark","permalink":"https://838819583.github.io/tags/spark/"}],"author":"林肥皂"},{"title":"Zookeeper安装配置","slug":"Zookeeper安装配置","date":"2022-04-14T04:01:22.000Z","updated":"2022-04-19T08:27:14.539Z","comments":true,"path":"2022/04/14/Zookeeper安装配置/","link":"","permalink":"https://838819583.github.io/2022/04/14/Zookeeper%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","excerpt":"zookeeper安装笔记","text":"zookeeper安装笔记 zookeeper的搭建主要分为三步1、解压tar包，设置环境变量2、修改zoo.cfg文件3、在对应的服务器上创建并添加myid 1、 解压zookeeper.tar.gz，添加环境变量123456tar -zxf zookeeper.tar.gz /opt/mv /opt/zookeeper-3.4.6 /opt/zookeepervim /etc/profileexport ZOOKEEPER_HOME=/opt/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/binsource /etc/profile 2、 修改zoo.cfg文件12345678cp zoo_sample.cfg zoo.cfgvim zoo.cfg#修改dataDir路径dataDir=/opt/zookeeper/data #指定内部通讯地址与选举leader地址server.1=master:2888:3888 server.2=slave1:2888:3888server.3=slave2:2888:3888 3、在对应的服务器上创建并添加myid12345678mkdir /opt/zookeeper/data#依据zoo.cfg的内容添加myid#masterecho &quot;1&quot; &gt;&gt; /opt/zookeeper/data/myid #slave1echo &quot;2&quot; &gt;&gt; /opt/zookeeper/data/myid #slave2echo &quot;3&quot; &gt;&gt; /opt/zookeeper/data/myid 配置完成后，在三台服务器执行zkServer.sh start启动zookeeper，执行zkServer.sh status查看状态","categories":[],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://838819583.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://838819583.github.io/tags/zookeeper/"}],"author":"林肥皂"},{"title":"2022职业技能大数据竞赛环境配置任务","slug":"2022职业技能大数据竞赛环境配置任务","date":"2022-04-13T10:43:03.000Z","updated":"2022-09-21T12:02:59.929Z","comments":true,"path":"2022/04/13/2022职业技能大数据竞赛环境配置任务/","link":"","permalink":"https://838819583.github.io/2022/04/13/2022%E8%81%8C%E4%B8%9A%E6%8A%80%E8%83%BD%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E4%BB%BB%E5%8A%A1/","excerpt":"2022职业技能大数据竞赛环境配置任务记录","text":"2022职业技能大数据竞赛环境配置任务记录 任务1：Hadoop完全分布式安装配置任务要求本环节需要使用root用户完成相关配置，安装Hadoop需要配置前置环境。命令中要求使用绝对路径，具体要求如下: 将Master节点JDK安装包解压并移动到&#x2F;usr&#x2F;local&#x2F;src&#x2F;路径(若路径不存在，则需新建)，将命令复制并粘贴至对应报告中; 修改&#x2F;root&#x2F;profile文件，设置JDK环境变量，配置完毕后在Master节点分别执行“java”和“javac”命令，将命令行执行结果分别截图并粘贴至对应报告中; 请完成host相关配置，将三个节点分别命名为master、slave1、slave2，并做免密登录，使用绝对路径从master复制JDK解压后的安装文件到slave1、slave2节点，并配置相关环境变量，将全部复制命令复制并粘贴至对应报告中; 在Master将Hadoop解压到&#x2F;opt目录下，并将解压包分发至slave1.slave2中,配置好相关环境，初始化Hadoop环境namenode，将初始化命令及初始化结果复制粘贴至对应报告中； 启动hadoop集群，查看master节点jps进程，将查看结果复制粘贴至对应报告中 1.1 解压jdk.tar.gz12mkdir /usr/local/src/tar -zxf jdk.tar.gz -C /usr/local/src/ 1.2 设置jdk环境变量执行java、javac在etc&#x2F;profile中追加 12export JAVA_HOME=/usr/local/src/jdk export PATH=$PATH:$JAVA_HOME/bin 最后执行source &#x2F;etc&#x2F;profile使之生效 1.3 设置hostname，关闭防火墙，完成免密登录123456789101112#设置hostnamevim /etc/hostname #生成ssh公钥ssh-keygen -t rsa -P &quot;&quot;#发放公钥ssh-copy-id 用户名@主机名#关闭防火墙systemctl stop firewalld.servicesystemctl disable firewalld.servicesystemctl status firewalld.service#尝试ssh登录ssh 主机名 1.4 解压hadoop.tar.gz，设置环境变量，完成hadoop配置，格式化NameNnode环境变量设置 12345678tar -zxf hadoop.tar.gz -C /opt/mv /opt/hadoop-2.7.7 /opt/hadoopvim /etc/profileexport HADOOP_CLASSPATH=`/opt/hadoop/bin/hadoop classpath`export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop/export HADOOP_HOME=/opt/hadoop/export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinsource /etc/profile core-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapreduce-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hadoop配置完成后，格式化namenode 1hadoop namenode -format 1.5 启动集群，查看进程情况12start-all.shjps master slave1 slave2 Namenode √ ResourceManager √ SecondaryNameNode √ Datanode √ √ √ NodeManager √ √ √ 任务2 Spark on Yarn安装配置任务要求本环节需要使用root用户完成相关配置，已安装Hadoop及需要配置前置环境，具体要求如下： 将scala包解压到&#x2F;usr&#x2F;路径，配置环境变量使其生效，将完整命令复制粘贴至对应报告中（若已安装，则可跳过）； 配置&#x2F;etc&#x2F;profile文件，设置Spark环境变量，并使环境变量生效将环境变量配置内容复制粘贴至对应报告中; 完成on yarn相关配置 使用spark on yarn的模式提交$SPARK_HOME&#x2F;examples&#x2F;jars&#x2F;spark-examples_2.11-2.1.1.jar,运行的主类为org.apache.spark.examples.SparkPi,将运行结果粘贴至对应报告中。 2.1 解压scala.tar.gz，配置环境变量1234567tar -zxf scala.tar.gz -C /usr/mv /usr/scala-2.1.1 /usr/scalavim /etc/profileexport SCALA_HOME=/usr/scala/export PATH=$PATH:$SCALA_HOME/binsource /etc/profilescala 2.2 解压spark.tar.gz，配置环境变量123456tar -zxf spark.tar.gz -C /usr/mv /usr/spark-2.1.1 /usr/scalavim /etc/profileexport SPARK_HOME=/opt/sparkexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbinsource /etc/profile 2.3 提交sparkPi1spark-submit --master yarn --class org.apache.spark.examples.SparkPi /opt/spark/examples/jars/spark-examples_2.11-2.1.1.jar 任务3 Flink on Yarn安装配置任务要求本环节需要使用root用户完成相关配置，已安装Hadoop及需要配置前置环境，具体要求如下： 将Flink包解压到路径&#x2F;opt目录下，将完整命令复制粘贴至对应报告中; 修改&#x2F;etc&#x2F;profile文件，设置Flink环境变量，并使环境变量生效将环境变量配置内容复制粘贴至对应报告中; 开启Hadoop集群，在yarn上以per job模式（即Job分离模式，不采用Session模式）运行&#x2F;examples&#x2F;batch&#x2F;WordCount.jar，将运行结果最后10行复制粘贴至对应报告中。示例：flink run -m yarn-cluster -p 2 -yjm 2G -ytm 2G &#x2F;examples&#x2F;batch&#x2F;WordCount.jar 3.1 解压Flink.tar.gz12tar -zxf flink.tar.gz -C /opt/mv /opt/flink-1.10.2 /opt/flink 3.2 配置环境变量1234vim /etc/profileexport FLINK_HOME=/opt/flinkexport PATH=$PATH:$FLINK_HOME/binsource /etc/profile 3.3 提交WordCount3.3.1 yarn prr-job方式1flink run -m yarn-cluster -p 2 -yjm 2G -ytm 2G ../examples/batch/WordCount.jar 3.3.2 yarn session方式12flink-session -dflink run -yjm 2G -ytm 2G ../examples/batch/WordCount.jar master:8080 yarn web ui 最终&#x2F;ect&#x2F;profile","categories":[],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://838819583.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"2022职业技能大数据竞赛","slug":"2022职业技能大数据竞赛","permalink":"https://838819583.github.io/tags/2022%E8%81%8C%E4%B8%9A%E6%8A%80%E8%83%BD%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"},{"name":"hadoop","slug":"hadoop","permalink":"https://838819583.github.io/tags/hadoop/"},{"name":"spark","slug":"spark","permalink":"https://838819583.github.io/tags/spark/"},{"name":"flink","slug":"flink","permalink":"https://838819583.github.io/tags/flink/"}],"author":"林肥皂"},{"title":"搜狗搜索日志实时展示项目(数据传输部分)","slug":"搜狗搜索日志实时展示项目-数据传输部分","date":"2022-01-03T16:52:52.000Z","updated":"2022-01-04T12:04:15.201Z","comments":true,"path":"2022/01/04/搜狗搜索日志实时展示项目-数据传输部分/","link":"","permalink":"https://838819583.github.io/2022/01/04/%E6%90%9C%E7%8B%97%E6%90%9C%E7%B4%A2%E6%97%A5%E5%BF%97%E5%AE%9E%E6%97%B6%E5%B1%95%E7%A4%BA%E9%A1%B9%E7%9B%AE-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93%E9%83%A8%E5%88%86/","excerpt":"用搜狗搜索日志进行简单的实时展示项目，本文为数据传输部分。","text":"用搜狗搜索日志进行简单的实时展示项目，本文为数据传输部分。 项目介绍本项目仍然使用搜狗官方公布的搜索数据，通过脚本模拟用户提交搜索，数据传输部分使用flume与kafka，在数据实时计算使用SparkStreaming,将计算结果存入mysql数据库，在数据展示方面使用Tomcat作为Web应用服务器，用websocket完成socket协议，从mysql中读取数据并通过Echarts展示。集群规划 数据准备使用搜狗搜索官方提供的搜索数据，其中包含”访问时间\\t用户ID\\t[查询词]\\t该URL在返回结果中的排名\\t用户点击的顺序号\\t用户点击的URL“搜狗搜索数据下载 数据传输部分思路本文使用shell脚本生成数据，主要实现每隔特定时间按行读取数据源并输出到特定文件，使用flume监听该文件并通过avro将数据聚集到一个flume，模拟生产环境中多个数据源的情况。最后通过这个该flume传输到kafka做消息队列，实现主题，方便多个消费者使用数据。 Shell脚本编写为了模拟用户搜索提交信息到服务器，本文使用shell脚本进行定时读取原搜索数据输出到新文件，该Shell脚本主要实现，按行读取初始数据，将数据中的”&#x2F;t”转换为”,”，每隔一定时间追加到特定文件。在hadoop1新建脚本create_Log.sh 12345if [ $# -ne 3 ] ; then echo &quot;usage $0 src_file dst_file frequence&quot; exitfiwhile read line ;do echo $line | tr &#x27;\\t&#x27; &#x27;,&#x27; &gt;&gt; $2 ; sleep $3 ; done &lt; $1 完成脚本编写后，添加执行权限。执行脚本即可实现特定文件每隔一段时间新增数据。使用nohup+&amp;将脚本挂在后台执行。 1234//执行脚本nohup create_Log 输入文件 输出文件 间隔时间 &amp;//查看输出结果tail -f 输出文件 监听文件flume配置在使用shell脚本成功将搜索数据按照一定时间间隔输入到新文件时，此时用flume监听该文件，得到新添加的数据。 首先先安装flume，在hadoop1,hadoop3下载并解压flume以及完成环境变量配置。flume下载地址 123456tar -zxf 解压包 -C 目标路径 //在.bashrc中添加export FLUME_HOME=flume路径export PATH=$PATH:$FLUME_HOME/bin//刷新环境变量文件。source ~/.bashrc 在hadoop1上要完成flume监听本地文件，并将得到的数据传输到hadoop3上的flumehadoop1新建配置文件flume-sogou.conf 12345678910111213141516171819202122232425# 设置agent name为a1，并绑定sources为r1，sinks为k1,channels为c1a1.sources = r1a1.sinks = k1a1.channels = c1# source配置为exec类型，执行command指令a1.sources.r1.type = execa1.sources.r1.command = tail -F 被监听文件a1.sources.r1.shell = /bin/bash -c# sink配置为avro类型，将sources产生的数据传输到hadoop3的flume上a1.sinks.k1.type = avroa1.sinks.k1.hostname = hadoop3a1.sinks.k1.port = 4141# channela1.channels.c1.type = memorya1.channels.c1.capacity = 10000a1.channels.c1.transactionCapacity = 10000a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 在hadoop3上要完成接收hadoop1中flume传输来的数据并传输到kafka特定主题上。hadoop3上新建配置文件flume-kafka.conf 12345678910111213141516171819202122232425# 设置agent name为a3，并绑定sources为r1，sinks为k1,channels为c1a3.sources = r1a3.sinks = k1a3.channels = c1# source为avro类型，用于接受hadoop1上传输来的数据a3.sources.r1.type = avroa3.sources.r1.bind = hadoop3a3.sources.r1.port = 4141# sink为KafkaSink，用于将source的数据传输到kafka上的weblog主题a3.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka3.sinks.k1.kafka.bootstrap.servers = hadoop1:9092,hadoop2:9092,hadoop3:9092a3.sinks.k1.kafka.topic = webloga3.sinks.k1.parseAsFlumeEvent = false# channela3.channels.c1.type = memorya3.channels.c1.capacity = 10000a3.channels.c1.transactionCapacity = 10000a3.sources.r1.channels = c1a3.sinks.k1.channel = c1 完成配置信息后，由于kafka还没安装，hadoop3的flume无法连接到kafka导致启动失败，并且由于hadoop1上的数据需要传输到hadoop3，出于avro的限制，要先将hadoop3上的flume任务启动，再将hadoop1上的flume任务启动，所以在未启动kafka服务之前，这两个flume任务无法使用。 kafka消费主题在三台虚拟机上安装kafka并设置环境变量。kafka下载地址 123456789tar -zxf 解压包 -C 目标路径 //配置kafka使用zookeeper服务器，修改conf文件夹内server.properties文件zookeeper.connet、log.dirs属性。log.dirs=flume路径/datazookeeper.connent=hadoop1:2181,hadoop2:2181,hadoop3:2181//在.bashrc中添加export KAFKA_HOME=kafka路径export PATH=$PATH:$KAFKA_HOME/bin//刷新环境变量文件。source ~/.bashrc 在安装完成后，确定zookeeper服务已经启动，在三台虚拟机上启动kafka服务 1bin/kafka-server-start.sh conf/server.properties 成功启动kafka服务后，就可以启动hadoop3上的flume以及hadoop1上的flume了，最后使用kafka-console-consumer.sh查看是否能够成功订阅weblog主题。 12345678//在hadoop3上nohup flune-ng --name a3 --conf-file flume-kafka &amp;//在hadoop1上nohup flume-ng --name a1 --conf-file flume-sogou.conf &amp;//任意虚拟机上订阅weblog主题kafka-console-consumer.sh --topic weblog --bootsrap-server hadoop3:9092","categories":[],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://838819583.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"搜狗搜索数据","slug":"搜狗搜索数据","permalink":"https://838819583.github.io/tags/%E6%90%9C%E7%8B%97%E6%90%9C%E7%B4%A2%E6%95%B0%E6%8D%AE/"},{"name":"flume","slug":"flume","permalink":"https://838819583.github.io/tags/flume/"},{"name":"kafka","slug":"kafka","permalink":"https://838819583.github.io/tags/kafka/"}],"author":"林肥皂"},{"title":"Hadoop集群安装与配置","slug":"Hadoop集群安装与配置","date":"2022-01-01T17:31:54.000Z","updated":"2022-04-14T07:41:06.410Z","comments":true,"path":"2022/01/02/Hadoop集群安装与配置/","link":"","permalink":"https://838819583.github.io/2022/01/02/Hadoop%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/","excerpt":"一般学习Hadoop中的组件，使用伪分布式就够用了，但是老师突然要求写一篇关于集群搭建的博客，那就写在自己的博客上吧，顺便丰富一下博客内容。","text":"一般学习Hadoop中的组件，使用伪分布式就够用了，但是老师突然要求写一篇关于集群搭建的博客，那就写在自己的博客上吧，顺便丰富一下博客内容。 Hadoop集群搭建大致步骤本文通过虚拟机搭建Hadoop集群，使用VMware Workstation Pro+CentOS7环境，均使用centos用户hadoop，搭建由三台主机构成的集群。搭建Hadoop集群主要就2个步骤，一是搭建三台能够相互通信的主机，二是在三台主机上安装Hadoop并完成相关配置。 搭建三台能够相互通信的主机 1.1 静态IP配置 1.2 Hosts映射配置 1.3 SSH免密登录配置 在三台主机上安装Hadoop并完成相关配置 2.1 JDK安装 2.2 Hadoop安装并修改配置文件 搭建三台能够相互通信的主机静态ip配置一般情况下，CentOS默认的IP获取方式是DHCP，但是对于集群来说，DHCP动态获取IP可能会导致集群间的通信不稳定，所以我们要设置静态IP。我们把虚拟机的网络连接模式更改为“桥接模式”，并获取宿主机的IP，三台虚拟机的IP要和宿主机处在同一网段下。更改连接模式和获取IP后，开始对CentOS进行静态IP的配置，具体配置文件在&#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens33 文件中，使用vim打开文件，完成如下配置：注意：配置的IP要和宿主机处在同一网段下，网关和宿主机相同。保存后输入systemctl restart network指令刷新网络设置，刷新后使用ifconfig查看IP是否已经更改。并尝试相互ping三台主机。 Hosts映射配置三台主机HostName和IP规划 Master 172.16.1.121 Slave1 172.16.1.122 Slave2 172.16.1.123 该步骤主要修改2个文件，&#x2F;etc&#x2F;hostname和&#x2F;etc&#x2F;hosts文件，其中hostname文件要根据主机的规划而有所不同，hosts文件三台主机一致即可。完成hostname和hosts配置后，尝试使用hostname相互ping三台主机。 SSH免密登录配置在Hadoop集群启动时会要求多次输入密码，非常麻烦，这时候就需要配置SSH免密登录了。在Master节点上输入该指令生成公钥 123ssh-keygen -t rsa (直接多次回车就行)cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 600 ~/.ssh/authorized_keys 将Master的公钥，通过scp指令发送到Slave1和Slave2的.ssh文件夹，即可完成Master免密登录Slave1和Slave2. 12scp ~/.ssh/id_rsa.pub hadoop@slave01:/home/hadoop/scp ~/.ssh/id_rsa.pub hadoop@slave02:/home/hadoop/ 再使用cat指令追加到authorized_keys下 12cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keysrm ~/id_rsa.pub 完成以上操作后，尝试在Master节点，SSH登录到Slave1和Slave2，能够直接登录就说明SSH免密登录配置完成。 在三台主机上安装Hadoop并完成相关配置JDK安装Hadoop是由Java语言编写，JDK是Java语言的软件开发工具包，想要运行Hadoop，必须得先安装JDK。下载对应的解压包，使用以下指令解压到指定目录 123tar -zxf 解压包 -C 指定目录例: tar -zxf jdk-8u291-linux-i586.tar.gz -C /home/hadoop/module/ 解压完成后，配置相应的环境变量： 12345vim ~/.bashrc添加以下内容：export JAVA_HOME=/home/hadoop/module/jdk1.8.0_291（JDK解压位置）export PATH=$&#123;JAVA_HOME&#125;/bin:$PATHsource ~/.bashrc（环境变量生效） 在配置完环境变量后，执行java -version查看java信息，返回以下信息说明JDK安装完成。 Hadoop安装并修改配置文件安装Hadoop只需要下载解压包，使用以下指令解压到指定目录即可 123tar -zxf 解压包 -C 指定目录例: tar -zxf hadoop-2.7.0.tar.gz -C /home/hadoop/module/ 成功解压Hadoop后，添加Hadoop环境变量，在~&#x2F;.bashrc添加以下内容 12export HADOOP_HOME=/home/hadoop/module/hadoop-2.7.0(Hadoop解压位置)export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 集群需要修改的配置文件比伪分布式要多，只少修改5个配置文件，分别为Slave，core-site.xml，hdfs-site.xml，mapred-site.xml（需要重命名），yarn-site.xml，具体配置文件存放在hadoop目录下的etc&#x2F;hadoop中。 一、Slave（输入DataNode的域名) 12Slave1Slave2 二、core-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/hadoop/module/hadoop-2.7.2/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://Master:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 三、hdfs-site.xml 123456789101112131415161718&lt;/configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;Master:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/hadoop/module/hadoop-2.7.2/tmp/dfs/name/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/hadoop/module/hadoop-2.7.2/tmp/dfs/data/&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 四、mapred-site.xml(需要将mapred-site.xml.template改名) 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;Master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;Master:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 五、yarn-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;Master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 在Master节点配置好后，可以用scp直接将Hadoop文件夹发送到Slave1、Slave2节点，免去重新修改配置文件的麻烦。 最后在Master上使用start-all.sh指令启动Hadoop集群，通过jps查看进程状态，如果成功启动以下进程，说明Hadoop集群配置成功。 Master jps NameNode SecondaryNameNode ResourceManager Slave1 jps DataNode NodeManager Slave2 jps DataNode NodeManager 如果有进程缺失，可以到hadoop目录下的log文件夹内读取日志查看具体错误，然后擅用搜索引擎解决。","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://838819583.github.io/tags/Hadoop/"},{"name":"大数据","slug":"大数据","permalink":"https://838819583.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"author":"林肥皂"},{"title":"Mapreduce计算搜狗搜索数据每小时话题统计","slug":"Mapreduce计算搜狗搜索数据每小时话题统计","date":"2021-12-31T16:00:00.000Z","updated":"2022-01-04T12:04:13.758Z","comments":true,"path":"2022/01/01/Mapreduce计算搜狗搜索数据每小时话题统计/","link":"","permalink":"https://838819583.github.io/2022/01/01/Mapreduce%E8%AE%A1%E7%AE%97%E6%90%9C%E7%8B%97%E6%90%9C%E7%B4%A2%E6%95%B0%E6%8D%AE%E6%AF%8F%E5%B0%8F%E6%97%B6%E8%AF%9D%E9%A2%98%E7%BB%9F%E8%AE%A1/","excerpt":"好久没更新了，希望在新的一年能坚持。","text":"好久没更新了，希望在新的一年能坚持。 统计目标使用mapreduce离线计算搜狗搜索数据中每小时的Top10话题，依据小时生成24个分区，输出格式为 数据准备使用搜狗搜索官方提供的搜索数据，其中包含”访问时间\\t用户ID\\t[查询词]\\t该URL在返回结果中的排名\\t用户点击的顺序号\\t用户点击的URL“在要完成每小时Top10话题的统计，需要提取数据中的访问时间、查询词两个字段。搜狗搜索数据下载 实现思路统计每小时的Top10话题，需要先根据访问时间进行分区，得到每个小时内所有的话题数，对该小时内相同的话题数进行累加，得到该小时所有话题及其总搜索数，得到总搜索数后，再对其进行排序并截取前10。由于先要进行分区再进行累加，所以一个mapreduce无法完成该任务，需要两个mapreduce，第一个mapreduce需要在map阶段获取时间字段与话题字段，在partition阶段完成每小时的分区，再在reduce阶段完成每小时内的累加。第二个mapreduce只需要在第一个的结果上完成分区，排序，取前10即可。 第一次mapreduce实现在第一次mapreduce阶段，主要实现三个功能，map阶段从输入数据中获取时间，话题字段。partition阶段要依据时间的小时实现分区，reduce阶段要统计每小时内话题的出现次数。 map输入的数据间由\\t分割，其中第一个字段与第三个字段为时间和话题，使用split函数根据”\\t”切分后，依据索引得到时间和话题，由于话题字段由[]包裹，使用replace函数去除即可。 12345678910public class NewsMapper extends Mapper&lt;LongWritable,Text,Text,Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = new String(value.getBytes(),0,value.getLength(),&quot;GBK&quot;); String[] spited = line.split(&quot;\\t&quot;); String date = spited[0]; String name = spited[2].replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;); context.write(new Text(name),new Text(date)); &#125;&#125; partitionmap阶段的输出格式为&lt;话题,时间&gt;，在partition阶段只需要读取时间字段，时间字段的格式为hh:mm:dd，使用split依据”：“切分得到小时，或使用substring截取前两个数字都行。 123456789public class NewsPartitioner extends Partitioner&lt;Text, Text&gt; &#123; @Override public int getPartition(Text name,Text inputdate, int i) &#123; String date = inputdate.toString(); int partitionNum = Integer.parseInt(date.split(&quot;:&quot;)[0]); return partitionNum; &#125;&#125; reducerreduce阶段主要实现话题点击数的累加，只需要对values进行遍历，每执行一次count数加一。最后与date拼接当作value写出。 123456789101112public class NewsReducer extends Reducer&lt;Text, Text,Text, Text&gt; &#123; String date; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; Long count=0L; for (Text value : values) &#123; count += 1; String date=value.toString(); &#125; context.write(key,new Text(date+&quot;\\t&quot;+count)); &#125;&#125; 输出结果 第二次mapreduce实现在第二次mapreduce阶段，主要要完成对每小时内话题的点击量的排序以及取前十的功能，由于仍要输出24个分片文件，故还要使用第一阶段自定义的partitioner。 map主要对第一阶段的输出文件读取话题，时间，话题点击数字段。由于map之后的Comparator阶段是对key进行排序，所以主要将话题点击数字段作为key阶段。 1234567891011public class NewsMapperSort extends Mapper&lt;LongWritable,Text, LongWritable,Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = new String(value.getBytes(),0,value.getLength(),&quot;UTF-8&quot;); String[] spited = line.split(&quot;\\t&quot;); String name = spited[0]; String date = spited[1]; Long count = Long.parseLong(spited[2]); context.write(new LongWritable(count),new Text(name+&quot;\\t&quot;+date)); &#125; partition思路与第一阶段相同 12345678public class NewsPartitioner extends Partitioner&lt;LongWritable, Text&gt; &#123; @Override public int getPartition(LongWritable longWritable, Text text, int i) &#123; String date = text.toString().split(&quot;\\t&quot;)[1]; int partitionNum=Integer.parseInt(date.split(&quot;:&quot;)[0]); return partitionNum; &#125;&#125; ComparatorComparator自带是采用顺序的方法排序，只需要倒反Comparable的返回值即可实现倒叙。注意重写compare的参数不能是object，会导致Compartor不生效。 12345678910public class NewsComparator extends WritableComparator &#123; public NewsComparator() &#123; super(LongWritable.class,true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; return -super.compare(a, b); &#125;&#125; reduce在输出结果中有要求将时间按小时取整，取出date使用substring提取小时拼接整点的分钟和秒即可。仅输出10条数据使用if条件判断输出条数即可。 12345678910111213141516public class NewsReducerSort extends Reducer&lt;LongWritable,Text,Text,LongWritable&gt; &#123; int i =0; @Override protected void reduce(LongWritable key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; if (i &lt; 10) &#123; for (Text value : values) &#123; String text = value.toString(); String name = text.split(&quot;\\t&quot;)[0]; String date = text.split(&quot;\\t&quot;)[1].substring(0, 2) + &quot;:00:00&quot;; context.write(new Text(date + &quot;\\t&quot; + name), key); i += 1; &#125; &#125; &#125;&#125; 输出结果","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://838819583.github.io/tags/Hadoop/"},{"name":"大数据","slug":"大数据","permalink":"https://838819583.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"搜狗搜索数据","slug":"搜狗搜索数据","permalink":"https://838819583.github.io/tags/%E6%90%9C%E7%8B%97%E6%90%9C%E7%B4%A2%E6%95%B0%E6%8D%AE/"}],"author":"林肥皂"},{"title":"Java API外网无法访问云主机HDFS datanode","slug":"Java-API外网无法访问云主机HDFS-datanode","date":"2021-06-08T16:01:07.000Z","updated":"2021-06-09T07:01:39.598Z","comments":true,"path":"2021/06/09/Java-API外网无法访问云主机HDFS-datanode/","link":"","permalink":"https://838819583.github.io/2021/06/09/Java-API%E5%A4%96%E7%BD%91%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE%E4%BA%91%E4%B8%BB%E6%9C%BAHDFS-datanode/","excerpt":"博主目前使用腾讯云+伪分布式的环境学习Hadoop框架，与一般的本地虚拟机+伪分布式环境相比，其中还是有一些区别的,会导致一些意外的错误。","text":"博主目前使用腾讯云+伪分布式的环境学习Hadoop框架，与一般的本地虚拟机+伪分布式环境相比，其中还是有一些区别的,会导致一些意外的错误。 问题发现今天在使用JAVA API读取HDFS文件时产生了两个错误第一个错误显示无法写入DataNode，示意我DataNode未工作，但是登录节点查看，显示DataNode工作正常。第二个错误，我已经获取了数据的字节大小，却显示数据块丢失，但是通过节点查看文件状态，仍表示正常。这两个报错真的相当具有迷惑性，在查阅了很多资料后，发现问题出在腾讯云+伪分布式的环境上。 错误原因HDFS的工作原理是客户端先向NameNode提交读写请求，NameNode向客户端返回物理存有数据DataNode节点的相关信息，客户端再向DataNode读写数据。根据第二个错误中可以获取数据的字节大小可以判断步骤1、2没有问题，问题出在步骤3，既客户端和DataNode的连接。通过查阅资料发现，在步骤2中，NameNode向客户端返回的是DataNode的IP地址，由于该IP是内网IP，导致客户端无法连接到DataNode，使得无法获取文件信息。 解决方法通过修改hdfs-site.xml配置文件，使得DadaNode和客户端之间采用域名的方式访问，解决内网IP无法连接的问题。在hdfs-site.xml中添加以下配置信息： 1234&lt;property&gt; &lt;name&gt;dfs.datanode.use.datanode.hostname&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 并在JAVA API初始化中，做相应的配置 1conf.set(&quot;dfs.client.use.datanode.hostname&quot;,&quot;true&quot;);","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://838819583.github.io/tags/Hadoop/"},{"name":"大数据","slug":"大数据","permalink":"https://838819583.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"HDFS","slug":"HDFS","permalink":"https://838819583.github.io/tags/HDFS/"},{"name":"错误记录","slug":"错误记录","permalink":"https://838819583.github.io/tags/%E9%94%99%E8%AF%AF%E8%AE%B0%E5%BD%95/"}],"author":"林肥皂"},{"title":"Hadoop框架总览","slug":"Hadoop框架总览","date":"2021-05-25T02:06:07.000Z","updated":"2021-06-09T07:00:50.558Z","comments":true,"path":"2021/05/25/Hadoop框架总览/","link":"","permalink":"https://838819583.github.io/2021/05/25/Hadoop%E6%A1%86%E6%9E%B6%E6%80%BB%E8%A7%88/","excerpt":"博主是大数据相关专业在学，目前已经跟着学校学习了一年多了，对大数据的一些框架有了初步的了解。之前也没有对知识进行梳理和总结。刚好，现在博客空空荡荡的，就先开一个坑。之后慢慢补充。","text":"博主是大数据相关专业在学，目前已经跟着学校学习了一年多了，对大数据的一些框架有了初步的了解。之前也没有对知识进行梳理和总结。刚好，现在博客空空荡荡的，就先开一个坑。之后慢慢补充。 Hadoop框架总览Hadoop是一个由Apache基金会所开发的分布式系统基础架构，作为大数据处理最基础的框架，有很多的大数据处理工具都是依赖于Hadoop框架，本文就先对Hadoop框架进行介绍。目前，Hadoop已经更新了3个大版本，其中Hadoop2.X相比Hadoop1.X做出了较大的更新，Hadoop1.X只有HDFS和MapReduce两个组件，由于资源调度和计算都由MapReduce处理，使得框架处理效率低下。在Hadoop2.X的更新中，添加了YARN组件，专门处理资源调度工作，而MapReduce则专门处理计算工作。Hadoop3.X主要对Hadoop2.X进行优化，提升了效率和兼容性。 Hadoop 组件介绍HDFSHDFS是Hadoop的分布式文件系统，主要处理大数据工作中的数据存储问题。使用HDFS具有以下优点 兼容性良好：使用java语言编写，只要服务器支持JVM就可以作为HDFS的节点。 扩展性良好：采用一个名称节点（NameNode）+多个数据节点（DataNode），数据节点只负责数据的存储，可以动态添加数据节点（既添加数据节点的时候可以不用重启集群） 对大文件支持好，速度快HDFS的设计目的就是针对大文件的处理，所以HDFS采用了切块的方式储存文件，默认每一个块为64M，相对于普通文件系统，在磁盘中寻址时间更短。由于是分布式存储，读取文件还有定位时间，HDFS在名称节点上存储了每个块的地址，可以快速的得到每个块的节点地址，使得定位时间减少。采用流数据读写，提高了数据的吞吐量。 数据安全性高采用一个名称节点+一个次名称节点（SeondaryNameNode）的结构，次名称节点会帮助名称节点进行热备份和元数据合并。对于每个文件块，在多台不同的服务器上有备份。 通过分布式的方式，将数据存储在不同的机器上，其优秀的可扩展性和兼容性，使得大量数据存储的成本降低。 HDFS 组件角色HDFS组件角色大致分为四类：NameNode，SecondaryNameNode，DataNode,Client。 NameNode： 接受客户端的读写请求 存储元数据信息 接受DataNode的心跳报告 分配数据块的储存节点 SecondaryNameNode: 备份元数据信息 帮助NameNode进行元数据合并 在紧急情况下，可以辅助恢复NameNode DataNode: 储存数据块 向NameNode发送心跳报告 处理客户端的读写请求 Client: Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作 文件切分。文件上传HDFS的时候，Client将文件切分成数据块 与NameNode交互，获取文件的位置信息 与DataNode交互，读取或者写入数据 HDFS 具体工作机制待更新 YARNYARN由于本人还没深入的学习，就先搁着了，等学习完再做更新 MapReduceMapReduce主要处理大主要处理大数据工作中的计算问题，在单台服务器算力有限的情况下，MapReduce采用分布式并行计算的方法，将一个任务划分为多个任务，分发到不同服务器进行并行计算，大大提升了计算效率。使用MapReduce计算具有以下特点 易于编程MapReduce提供了分布式并行计算的框架，开发者只需要把注意力集中在业务逻辑的编写上即可，无需考虑如何将任务划分，如何分发到不同的服务器上运行这些底层的问题，这些功能MapReduce框架已经实现了。 不适合流式计算流式计算的输入数据的动态的，而MapReduce出于自身设计原因，会产生许多中间数据文件，并依赖这些文件继续计算。如果是输入数据是动态的，输入数据和中间数据不同步，也就会导致计算结果不同。所以MapReduce不适合流式计算 不适合实时计算MapReduce会对输入数据进行切分、产生中间数据、排序、数据压缩、持久化等操作，效率低下，使用MapReduce计算时间相对较长，不适合用于实时计算。 不适合有依赖关系的计算MapReduce是分布式并行计算框架，适用于并行计算。在一次MapReduce中，某个任务的输入是另一个任务的输出时，既两个任务是串行关系，就需要把前一个任务的输出全部写入到磁盘，再进行下一个任务，这样会产生大量的磁盘IO，导致效率下降。 MapReduce大致工作流程MapReduce主要由Map阶段和Reduce阶段构成，其中Map的输出会作为Reduce的输入，先进行Map阶段，再进行Reduce阶段。围绕这两个阶段，还有一些其他的流程。大致流程图如下： MapReduce具体工作机制待更新","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://838819583.github.io/tags/Hadoop/"},{"name":"大数据","slug":"大数据","permalink":"https://838819583.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"author":"林肥皂"},{"title":"关于","slug":"about","date":"2021-05-24T06:25:00.000Z","updated":"2022-10-25T03:45:38.317Z","comments":true,"path":"/about.html","link":"","permalink":"https://838819583.github.io/about.html","excerpt":"","text":"关于博客该博客主要记录本人的学习文章，总结一些知识或是记录一些常用操作，由于本人好吃懒做，该博客更新时间和频率完全随缘。 关于本人博主是某不知名大专的大二学生，大数据相关专业在熬，喜欢折腾，主要学习大数据相关组件与python，希望早日成为一名大数据工程师。先写这么多，以后想到什么再更新吧（我实在是太懒啦） 待更新","categories":[],"tags":[],"author":"林肥皂"}],"categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://838819583.github.io/tags/linux/"},{"name":"开源工具","slug":"开源工具","permalink":"https://838819583.github.io/tags/%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7/"},{"name":"python","slug":"python","permalink":"https://838819583.github.io/tags/python/"},{"name":"pyecharts","slug":"pyecharts","permalink":"https://838819583.github.io/tags/pyecharts/"},{"name":"可视化","slug":"可视化","permalink":"https://838819583.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"大数据","slug":"大数据","permalink":"https://838819583.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"flink","slug":"flink","permalink":"https://838819583.github.io/tags/flink/"},{"name":"spark","slug":"spark","permalink":"https://838819583.github.io/tags/spark/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://838819583.github.io/tags/zookeeper/"},{"name":"2022职业技能大数据竞赛","slug":"2022职业技能大数据竞赛","permalink":"https://838819583.github.io/tags/2022%E8%81%8C%E4%B8%9A%E6%8A%80%E8%83%BD%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"},{"name":"hadoop","slug":"hadoop","permalink":"https://838819583.github.io/tags/hadoop/"},{"name":"搜狗搜索数据","slug":"搜狗搜索数据","permalink":"https://838819583.github.io/tags/%E6%90%9C%E7%8B%97%E6%90%9C%E7%B4%A2%E6%95%B0%E6%8D%AE/"},{"name":"flume","slug":"flume","permalink":"https://838819583.github.io/tags/flume/"},{"name":"kafka","slug":"kafka","permalink":"https://838819583.github.io/tags/kafka/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://838819583.github.io/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"https://838819583.github.io/tags/HDFS/"},{"name":"错误记录","slug":"错误记录","permalink":"https://838819583.github.io/tags/%E9%94%99%E8%AF%AF%E8%AE%B0%E5%BD%95/"}]}